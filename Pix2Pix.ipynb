{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ATTENTION: Ensure that the dataset directories are changed to the appropriate directories if testing."
      ],
      "metadata": {
        "id": "sFpnMbU9_J7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3vwX_4SOARy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zj1-9rfHtko"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, num_input_channels = 1, num_output_channels = 3, num_features = 64):\n",
        "    super(Generator, self).__init__()\n",
        "    # Padding --> p = ((s - 1) * (input_size - 1) + k - input_size) / 2, where k is the kernel size and s is the stride.\n",
        "    p = ((2 - 1) * (num_input_channels - 1) + 4 - num_input_channels) / 2\n",
        "  \n",
        "    self.enc1 = self._encoder(num_input_channels, num_features, kernel_size = 4, stride = 2, padding = 1, first_layer = True)\n",
        "    self.enc2 = self._encoder(num_features, num_features * 2, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "    self.enc3 = self._encoder(num_features * 2, num_features * 4, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "    self.enc4 = self._encoder(num_features * 4, num_features * 8, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "    self.enc5 = self._encoder(num_features * 8, num_features * 8, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "    self.enc6 = self._encoder(num_features * 8, num_features * 8, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "    self.enc7 = self._encoder(num_features * 8, num_features * 8, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "\n",
        "    # Bottleneck:\n",
        "    self.bottleneck = nn.Conv2d(num_features * 8, num_features * 8, kernel_size = 4, stride = 2, padding = 1, padding_mode = \"reflect\")\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.dec1 = self._decoder(num_features * 8, num_features * 8, kernel_size = 4, stride = 2, padding = 1, use_dropout = True)\n",
        "    self.dec2 = self._decoder(num_features * 8 * 2, num_features * 8, kernel_size = 4, stride = 2, padding = 1, use_dropout = True)\n",
        "    self.dec3 = self._decoder(num_features * 8 * 2, num_features * 8, kernel_size = 4, stride = 2, padding = 1, use_dropout = True)\n",
        "    self.dec4 = self._decoder(num_features * 8 * 2, num_features * 8, kernel_size = 4, stride = 2, padding = 1, use_dropout = False)\n",
        "    self.dec5 = self._decoder(num_features * 8 * 2, num_features * 4, kernel_size = 4, stride = 2, padding = 1, use_dropout = False)\n",
        "    self.dec6 = self._decoder(num_features * 4 * 2, num_features * 2, kernel_size = 4, stride = 2, padding = 1, use_dropout = False)\n",
        "    self.dec7 = self._decoder(num_features * 2 * 2, num_features, kernel_size = 4, stride = 2, padding = 1, use_dropout = False)\n",
        "    self.dec8 = nn.ConvTranspose2d(num_features * 2, num_output_channels, kernel_size = 4, stride = 2, padding = 1)\n",
        "    self.tanh = nn.Tanh() # For colorization, sigmoid can be more suitable as the values would be from 0 to 1 for the color channels, rather than Tanh's [-1, 1] range that\n",
        "    # can lead to a steeper gradient IF THE COLORS ARE NORMALIZED. NOT IN THIS CASE. \n",
        "    \n",
        "  \n",
        "\n",
        "  def _encoder(self, in_channels, out_channels, kernel_size, stride, padding, first_layer = True):\n",
        "    if first_layer == True:\n",
        "      # Batch normalization is not applied on the first layer in the encoder. \n",
        "      return nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias = False, padding_mode = \"reflect\"),\n",
        "          nn.LeakyReLU(0.2)\n",
        "       )\n",
        "    if first_layer == False:\n",
        "      return nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias = False, padding_mode = \"reflect\"),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.LeakyReLU(0.2)\n",
        "       )\n",
        "  \n",
        "  def _decoder(self, in_channels, out_channels, kernel_size, stride, padding, use_dropout = True):\n",
        "    # All ReLUs in the decoder are NOT leaky.\n",
        "    if use_dropout == True:\n",
        "      return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.Dropout2d(0.5),\n",
        "        nn.ReLU(0.2)\n",
        "      )\n",
        "    if use_dropout == False:\n",
        "      return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(0.2)\n",
        "      )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    c1 = self.enc1(x)\n",
        "    c2 = self.enc2(c1)\n",
        "    c3 = self.enc3(c2)\n",
        "    c4 = self.enc4(c3)\n",
        "    c5 = self.enc5(c4)\n",
        "    c6 = self.enc6(c5)\n",
        "    c7 = self.enc7(c6)\n",
        "    bottle = self.relu(self.bottleneck(c7))\n",
        "    d1 = self.dec1(bottle)\n",
        "    d2 = self.dec2(torch.cat([d1, c7], dim = 1))\n",
        "    d3 = self.dec3(torch.cat([d2, c6], dim = 1))\n",
        "    d4 = self.dec4(torch.cat([d3, c5], dim = 1))\n",
        "    d5 = self.dec5(torch.cat([d4, c4], dim = 1))\n",
        "    d6 = self.dec6(torch.cat([d5, c3], dim = 1))\n",
        "    d7 = self.dec7(torch.cat([d6, c2], dim = 1))\n",
        "    d8 = self.dec8(torch.cat([d7, c1], dim = 1))\n",
        "    final = self.tanh(d8)\n",
        "\n",
        "    return final\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v3jLz_5Nvyy"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "  x = torch.randn((1, 3, 256, 256))\n",
        "  model = Generator(num_input_channels = 3, num_output_channels = 3, num_features = 64)\n",
        "  preds = model(x)\n",
        "  print(preds.shape)\n",
        "\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo7ng3u8QgCA"
      },
      "outputs": [],
      "source": [
        "# 286 x 286 discriminator architecture:\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, input_channels = 3, features = [64, 128, 256, 512]):\n",
        "    super(Discriminator, self).__init__()\n",
        "    p = ((2 - 1) * (input_channels - 1) + 4 - input_channels) / 2\n",
        "    self.l1 = self._block(input_channels * 2, features[0], kernel_size = 4, stride = 2, padding = 1, first_layer = True)\n",
        "    layers = []\n",
        "    in_channels = features[0]\n",
        "    for feature in features[1:]:\n",
        "      layers.append(\n",
        "          self._block(in_channels, feature, kernel_size = 4, stride = 1 if feature == features[-1] else 2, padding = 1, first_layer = False)\n",
        "      )\n",
        "      in_channels = feature\n",
        "    layers.append(self._block(512, 512, kernel_size = 4, stride = 1, padding = 1, first_layer = False))\n",
        "    layers.append(self._block(512, 512, kernel_size = 4, stride = 1, padding = 1, first_layer = False))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "    self.final = nn.Conv2d(features[-1], 1, kernel_size = 1, stride = 1, padding = 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "# Standard convolutional layer.\n",
        "  def _block(self, in_channels, out_channels, kernel_size, stride, padding, first_layer = True):\n",
        "    if first_layer == True:\n",
        "      return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode = \"reflect\"),\n",
        "        nn.LeakyReLU(0.2)\n",
        "        )\n",
        "    if first_layer == False:\n",
        "      return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode = \"reflect\"),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2)\n",
        "      )  \n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    x = torch.cat([x, y], dim = 1)\n",
        "    x = self.l1(x)\n",
        "    x = self.model(x)\n",
        "    x = self.final(x)\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIBToMlOFltZ"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "  x = torch.randn((1, 3, 286, 286))\n",
        "  y = torch.randn((1, 3, 286, 286))\n",
        "  model = Discriminator()\n",
        "  preds = model(x, y)\n",
        "  print(preds.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjpTg881MO0S"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qdo_vg-9pdVF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'atasgeld'\n",
        "os.environ['KAGGLE_KEY'] = '531dce420032bafdfc6a47e0ce5e1fcf'\n",
        "\n",
        "!kaggle datasets download -d ashwingupta3012/human-faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-jRxwRAM6pv"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import zipfile\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/FINALDATA/higher.zip\"\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_file:\n",
        "  zip_file.extractall('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnLNv1AqqCAE"
      },
      "outputs": [],
      "source": [
        "transform = [\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, ), (0.5, ), (0.5,))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RE3Y4u64Xjy"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "def save_some_examples(gen, val_loader, epoch, folder):\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = []\n",
        "        outputs = []\n",
        "        for i in range(8):\n",
        "            x, y = val_loader.dataset[np.random.randint(len(val_loader.dataset))]\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_fake = gen(x.unsqueeze(0))\n",
        "            y_fake = y_fake.squeeze(0) * 0.5 + 0.5  # remove normalization\n",
        "            inputs.append(x * 0.5 + 0.5)\n",
        "            outputs.append(y_fake)\n",
        "        inputs_grid = vutils.make_grid(inputs, nrow=4, normalize=True, scale_each=True)\n",
        "        outputs_grid = vutils.make_grid(outputs, nrow=4, normalize=True, scale_each=True)\n",
        "        grid = torch.cat([inputs_grid, outputs_grid], dim=1)\n",
        "        vutils.save_image(grid, folder + f\"/examples_{epoch}.png\")\n",
        "    gen.train()\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location= device)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D6JZSq_YXuf"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "class ImageDataset_color(Dataset):\n",
        "    def __init__(self, root, transforms_=None, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        self.files = sorted(glob.glob(os.path.join(root) + \"/*.*\"))\n",
        "        print(f\"Number of files found: {len(self.files)}\")\n",
        "        print(self.files[:10])\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_A = cv2.imread(self.files[index % len(self.files)])\n",
        "        img_A = cv2.cvtColor(img_A, cv2.COLOR_BGR2RGB)\n",
        "        img_B = cv2.cvtColor(cv2.cvtColor(img_A, cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
        "        img_A = Image.fromarray(np.array(img_A), \"RGB\")\n",
        "        img_B = Image.fromarray(np.array(img_B), \"RGB\")\n",
        "        if np.random.random() < 0.5:\n",
        "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
        "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
        "\n",
        "        img_A = self.transform(img_A)\n",
        "        img_B = self.transform(img_B)\n",
        "\n",
        "        return img_B, img_A\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUARz-OA2VOV"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter setup:\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "learning_rate = 2e-4 # A smaller learning rate should be used when training on a new dataset because we want to make small adjustments\n",
        "# for the new dataset. \n",
        "batch_size = 4\n",
        "num_workers = 2\n",
        "image_size = 256\n",
        "channels_img = 3\n",
        "l1_lambda = 100\n",
        "num_epochs = 100\n",
        "load_model = True\n",
        "save_model = True\n",
        "checkpoint_disc = \"/content/drive/MyDrive/FINALDATA/disc.pth.tar\"\n",
        "checkpoint_gen = \"/content/drive/MyDrive/FINALDATA/gen.pth.tar\"\n",
        "\n",
        "both_transform = A.Compose([\n",
        "    A.Resize(width=256, height=256),\n",
        "    A.HorizontalFlip(p=0.5)\n",
        "], additional_targets={\"image0\":\"image\"})\n",
        "\n",
        "transform_only_input = A.Compose(\n",
        "    [\n",
        "     A.ColorJitter(p=0.2),\n",
        "     A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
        "     A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n",
        "     ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_only_mask = A.Compose(\n",
        "    [\n",
        "     A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
        "     A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n",
        "     ToTensorV2()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz98gmIoM09l"
      },
      "outputs": [],
      "source": [
        "train_dataset = ImageDataset_color(root = '/content/framesito', transforms_ = transform)\n",
        "val_dataset = ImageDataset_color(root = '/content/higher', transforms_ = transform)\n",
        "\n",
        "# Show some images from the train dataset\n",
        "for i in range(3):\n",
        "    input_image, target_image = train_dataset[i]\n",
        "    input_image = np.transpose(input_image, (1, 2, 0))\n",
        "    target_image = np.transpose(target_image, (1, 2, 0))\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(input_image)\n",
        "    axes[0].set_title('Input Image')\n",
        "    axes[1].imshow(target_image)\n",
        "    axes[1].set_title('Target Image')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7tINP5X4ED0"
      },
      "outputs": [],
      "source": [
        "# Training:\n",
        "\n",
        "disc = Discriminator(input_channels = 3).to(device)\n",
        "gen = Generator(num_input_channels = 3, num_output_channels = 3).to(device)\n",
        "opt_disc = optim.Adam(disc.parameters(), lr = learning_rate, betas = (0.5, 0.999))\n",
        "opt_gen = optim.Adam(gen.parameters(), lr = learning_rate, betas = (0.5, 0.999))\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "\n",
        "if load_model == True:\n",
        "  load_checkpoint(checkpoint_gen, gen, opt_gen, learning_rate)\n",
        "  load_checkpoint(checkpoint_disc, disc, opt_disc, learning_rate)\n",
        "\n",
        "\n",
        "#train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
        "g_scaler = torch.cuda.amp.GradScaler()\n",
        "d_scaler = torch.cuda.amp.GradScaler()\n",
        "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SILpy3OG71SH"
      },
      "outputs": [],
      "source": [
        "def train_fn(disc, gen, loader, opt_disc, opt_gen, l1, bce_loss, g_scaler, d_scaler):\n",
        "  loop = tqdm(loader, leave = True)\n",
        "\n",
        "  for index, (x, y) in enumerate(loop):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Train discriminator:\n",
        "    with torch.cuda.amp.autocast(): # Float 16.\n",
        "      y_fake = gen(x)\n",
        "      d_real = disc(x, y)\n",
        "      d_fake = disc(x, y_fake) # \n",
        "      d_real_loss = bce_loss(d_real, torch.ones_like(d_real))\n",
        "      d_fake_loss = bce_loss(d_fake, torch.zeros_like(d_fake))\n",
        "      d_loss = (d_real_loss + d_fake_loss) / 2 # Divide by two to make the discriminator train 'slower' than the generator. \n",
        "    \n",
        "    disc.zero_grad()\n",
        "    d_scaler.scale(d_loss).backward(retain_graph = True)\n",
        "    d_scaler.step(opt_disc)\n",
        "    d_scaler.update()\n",
        "\n",
        "    # Train generator:\n",
        "    with torch.cuda.amp.autocast():\n",
        "      d_fake = disc(x, y_fake)\n",
        "      g_fake_loss = bce_loss(d_fake, torch.ones_like(d_fake)) # Trick the discriminator into thinking that these are real images. \n",
        "      l1 = l1_loss(y_fake, y) * l1_lambda\n",
        "      g_loss = g_fake_loss + l1\n",
        "    \n",
        "    opt_gen.zero_grad() # Clear weights.\n",
        "    g_scaler.scale(g_loss).backward()\n",
        "    g_scaler.step(opt_gen)\n",
        "    g_scaler.update()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQwBwWZj4E0W"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "  train_fn(disc, gen, train_loader, opt_disc, opt_gen, l1_loss, bce_loss, g_scaler, d_scaler)\n",
        "\n",
        "  if save_model and epoch % 1 == 0:\n",
        "    save_checkpoint(gen, opt_gen, filename = checkpoint_gen)\n",
        "    save_checkpoint(disc, opt_disc, filename = checkpoint_disc)\n",
        "  \n",
        "  save_some_examples(gen, val_loader, epoch, folder = \"./evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LubuEnJfJqGD"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def test_image(gen, img_dir, folder):\n",
        "  \"\"\"\n",
        "  Allows for testing of a single image by passing it in directly via its directory\n",
        "  and converting it to a PIL-type image. \n",
        "  The image is transformed and resized to the appropriate size.\n",
        "  Then, the input tensor is passed into the generator, which generates the 'fake'\n",
        "  colorized image and saves it in the testing folder. \n",
        "  \"\"\"\n",
        "  input_image = Image.open(img_dir).convert('RGB')\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize((1024, 1024)),\n",
        "      transforms.ToTensor(), \n",
        "      transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
        "  ])\n",
        "  input_tensor = transform(input_image).unsqueeze(0).to(device) # unsqueeze(0) \n",
        "  # adds a new dimension to the tensor at position 0. \n",
        "  # Since the transform function returns a tensor of shape (channels, height, width), we\n",
        "  # want the resulting shape to be (1, height, width) so that only one image is fed (as a batch size of 1).\n",
        "  gen.eval()\n",
        "  with torch.no_grad():\n",
        "    y_fake = gen(input_tensor)\n",
        "    y_fake = y_fake * 0.5 + 0.5 # Remove normalization.\n",
        "    save_image(y_fake, folder + f\"/colorized_image.png\")\n",
        "    save_image(input_tensor.squeeze() * 0.5 + 0.5, folder + f\"/original_image.png\")\n",
        "  gen.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTPFejVsMIsz"
      },
      "outputs": [],
      "source": [
        "test_image(gen, img_dir = \"/content/5d4d43d585600a0462410713.jpg\", folder = \"testing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dlu7MxOJTWzv"
      },
      "outputs": [],
      "source": [
        "save_some_examples(gen, val_loader, 1, \"./evaluation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR_zxMufZRYr"
      },
      "outputs": [],
      "source": [
        "load_checkpoint()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}