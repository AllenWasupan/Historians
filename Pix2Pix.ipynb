{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3vwX_4SOARy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zj1-9rfHtko"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, num_input_channels = 1, num_output_channels = 3, num_features = 64):\n",
        "    super(Generator, self).__init__()\n",
        "    # Padding --> p = ((s - 1) * (input_size - 1) + k - input_size) / 2, where k is the kernel size and s is the stride.\n",
        "    p = ((2 - 1) * (num_input_channels - 1) + 4 - num_input_channels) / 2\n",
        "  \n",
        "    self.enc1 = self._encoder(num_input_channels, num_features, kernel_size = 4, stride = 2, padding = 1, first_layer = True)\n",
        "    self.enc2 = self._encoder(num_features, num_features * 2, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "    self.enc3 = self._encoder(num_features * 2, num_features * 4, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "    self.enc4 = self._encoder(num_features * 4, num_features * 8, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "    self.enc5 = self._encoder(num_features * 8, num_features * 8, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "    self.enc6 = self._encoder(num_features * 8, num_features * 8, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "    self.enc7 = self._encoder(num_features * 8, num_features * 8, kernel_size = 4, stride = 2, padding = 1, first_layer = False)\n",
        "\n",
        "    # Bottleneck:\n",
        "    self.bottleneck = nn.Conv2d(num_features * 8, num_features * 8, kernel_size = 4, stride = 2, padding = 1, padding_mode = \"reflect\")\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.dec1 = self._decoder(num_features * 8, num_features * 8, kernel_size = 4, stride = 2, padding = 1, use_dropout = True)\n",
        "    self.dec2 = self._decoder(num_features * 8 * 2, num_features * 8, kernel_size = 4, stride = 2, padding = 1, use_dropout = True)\n",
        "    self.dec3 = self._decoder(num_features * 8 * 2, num_features * 8, kernel_size = 4, stride = 2, padding = 1, use_dropout = True)\n",
        "    self.dec4 = self._decoder(num_features * 8 * 2, num_features * 8, kernel_size = 4, stride = 2, padding = 1, use_dropout = False)\n",
        "    self.dec5 = self._decoder(num_features * 8 * 2, num_features * 4, kernel_size = 4, stride = 2, padding = 1, use_dropout = False)\n",
        "    self.dec6 = self._decoder(num_features * 4 * 2, num_features * 2, kernel_size = 4, stride = 2, padding = 1, use_dropout = False)\n",
        "    self.dec7 = self._decoder(num_features * 2 * 2, num_features, kernel_size = 4, stride = 2, padding = 1, use_dropout = False)\n",
        "    self.dec8 = nn.ConvTranspose2d(num_features * 2, num_output_channels, kernel_size = 4, stride = 2, padding = 1)\n",
        "    self.tanh = nn.Tanh() # For colorization, sigmoid can be more suitable as the values would be from 0 to 1 for the color channels, rather than Tanh's [-1, 1] range that\n",
        "    # can lead to a steeper gradient IF THE COLORS ARE NORMALIZED. NOT IN THIS CASE. \n",
        "    \n",
        "  \n",
        "\n",
        "  def _encoder(self, in_channels, out_channels, kernel_size, stride, padding, first_layer = True):\n",
        "    if first_layer == True:\n",
        "      # Batch normalization is not applied on the first layer in the encoder. \n",
        "      return nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias = False, padding_mode = \"reflect\"),\n",
        "          nn.LeakyReLU(0.2)\n",
        "       )\n",
        "    if first_layer == False:\n",
        "      return nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias = False, padding_mode = \"reflect\"),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.LeakyReLU(0.2)\n",
        "       )\n",
        "  \n",
        "  def _decoder(self, in_channels, out_channels, kernel_size, stride, padding, use_dropout = True):\n",
        "    # All ReLUs in the decoder are NOT leaky.\n",
        "    if use_dropout == True:\n",
        "      return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.Dropout2d(0.5),\n",
        "        nn.ReLU(0.2)\n",
        "      )\n",
        "    if use_dropout == False:\n",
        "      return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(0.2)\n",
        "      )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    c1 = self.enc1(x)\n",
        "    c2 = self.enc2(c1)\n",
        "    c3 = self.enc3(c2)\n",
        "    c4 = self.enc4(c3)\n",
        "    c5 = self.enc5(c4)\n",
        "    c6 = self.enc6(c5)\n",
        "    c7 = self.enc7(c6)\n",
        "    bottle = self.relu(self.bottleneck(c7))\n",
        "    d1 = self.dec1(bottle)\n",
        "    d2 = self.dec2(torch.cat([d1, c7], dim = 1))\n",
        "    d3 = self.dec3(torch.cat([d2, c6], dim = 1))\n",
        "    d4 = self.dec4(torch.cat([d3, c5], dim = 1))\n",
        "    d5 = self.dec5(torch.cat([d4, c4], dim = 1))\n",
        "    d6 = self.dec6(torch.cat([d5, c3], dim = 1))\n",
        "    d7 = self.dec7(torch.cat([d6, c2], dim = 1))\n",
        "    d8 = self.dec8(torch.cat([d7, c1], dim = 1))\n",
        "    final = self.tanh(d8)\n",
        "\n",
        "    return final\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v3jLz_5Nvyy",
        "outputId": "2d230643-a1d7-4fc4-d592-62e6b208e3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "  x = torch.randn((1, 3, 256, 256))\n",
        "  model = Generator(num_input_channels = 3, num_output_channels = 3, num_features = 64)\n",
        "  preds = model(x)\n",
        "  print(preds.shape)\n",
        "\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo7ng3u8QgCA"
      },
      "outputs": [],
      "source": [
        "# 286 x 286 discriminator architecture:\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, input_channels = 3, features = [64, 128, 256, 512]):\n",
        "    super(Discriminator, self).__init__()\n",
        "    p = ((2 - 1) * (input_channels - 1) + 4 - input_channels) / 2\n",
        "    self.l1 = self._block(input_channels * 2, features[0], kernel_size = 4, stride = 2, padding = 1, first_layer = True)\n",
        "    layers = []\n",
        "    in_channels = features[0]\n",
        "    for feature in features[1:]:\n",
        "      layers.append(\n",
        "          self._block(in_channels, feature, kernel_size = 4, stride = 1 if feature == features[-1] else 2, padding = 1, first_layer = False)\n",
        "      )\n",
        "      in_channels = feature\n",
        "    layers.append(self._block(512, 512, kernel_size = 4, stride = 1, padding = 1, first_layer = False))\n",
        "    layers.append(self._block(512, 512, kernel_size = 4, stride = 1, padding = 1, first_layer = False))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "    self.final = nn.Conv2d(features[-1], 1, kernel_size = 1, stride = 1, padding = 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "# Standard convolutional layer.\n",
        "  def _block(self, in_channels, out_channels, kernel_size, stride, padding, first_layer = True):\n",
        "    if first_layer == True:\n",
        "      return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode = \"reflect\"),\n",
        "        nn.LeakyReLU(0.2)\n",
        "        )\n",
        "    if first_layer == False:\n",
        "      return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode = \"reflect\"),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2)\n",
        "      )  \n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    x = torch.cat([x, y], dim = 1)\n",
        "    x = self.l1(x)\n",
        "    x = self.model(x)\n",
        "    x = self.final(x)\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIBToMlOFltZ"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "  x = torch.randn((1, 3, 286, 286))\n",
        "  y = torch.randn((1, 3, 286, 286))\n",
        "  model = Discriminator()\n",
        "  preds = model(x, y)\n",
        "  print(preds.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "JjpTg881MO0S",
        "outputId": "bea77454-012e-4b5c-ad18-4fa8f7d489ab"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6429206cf091>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-534301d055e2>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m286\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m286\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m286\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m286\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Discriminator' is not defined"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qdo_vg-9pdVF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'atasgeld'\n",
        "os.environ['KAGGLE_KEY'] = '531dce420032bafdfc6a47e0ce5e1fcf'\n",
        "\n",
        "!kaggle datasets download -d ashwingupta3012/human-faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-jRxwRAM6pv"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import zipfile\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/FINALDATA/higher.zip\"\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_file:\n",
        "  zip_file.extractall('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnLNv1AqqCAE"
      },
      "outputs": [],
      "source": [
        "transform = [\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, ), (0.5, ), (0.5,))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RE3Y4u64Xjy"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "def save_some_examples(gen, val_loader, epoch, folder):\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = []\n",
        "        outputs = []\n",
        "        for i in range(8):\n",
        "            x, y = val_loader.dataset[np.random.randint(len(val_loader.dataset))]\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_fake = gen(x.unsqueeze(0))\n",
        "            y_fake = y_fake.squeeze(0) * 0.5 + 0.5  # remove normalization\n",
        "            inputs.append(x * 0.5 + 0.5)\n",
        "            outputs.append(y_fake)\n",
        "        inputs_grid = vutils.make_grid(inputs, nrow=4, normalize=True, scale_each=True)\n",
        "        outputs_grid = vutils.make_grid(outputs, nrow=4, normalize=True, scale_each=True)\n",
        "        grid = torch.cat([inputs_grid, outputs_grid], dim=1)\n",
        "        vutils.save_image(grid, folder + f\"/examples_{epoch}.png\")\n",
        "    gen.train()\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location= device)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D6JZSq_YXuf"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "class ImageDataset_color(Dataset):\n",
        "    def __init__(self, root, transforms_=None, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        self.files = sorted(glob.glob(os.path.join(root) + \"/*.*\"))\n",
        "        print(f\"Number of files found: {len(self.files)}\")\n",
        "        print(self.files[:10])\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_A = cv2.imread(self.files[index % len(self.files)])\n",
        "        img_A = cv2.cvtColor(img_A, cv2.COLOR_BGR2RGB)\n",
        "        img_B = cv2.cvtColor(cv2.cvtColor(img_A, cv2.COLOR_RGB2GRAY), cv2.COLOR_GRAY2RGB)\n",
        "        img_A = Image.fromarray(np.array(img_A), \"RGB\")\n",
        "        img_B = Image.fromarray(np.array(img_B), \"RGB\")\n",
        "        if np.random.random() < 0.5:\n",
        "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
        "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
        "\n",
        "        img_A = self.transform(img_A)\n",
        "        img_B = self.transform(img_B)\n",
        "\n",
        "        return img_B, img_A\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUARz-OA2VOV"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter setup:\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "learning_rate = 2e-4 # A smaller learning rate should be used when training on a new dataset because we want to make small adjustments\n",
        "# for the new dataset. \n",
        "batch_size = 4\n",
        "num_workers = 2\n",
        "image_size = 256\n",
        "channels_img = 3\n",
        "l1_lambda = 100\n",
        "num_epochs = 100\n",
        "load_model = True\n",
        "save_model = True\n",
        "checkpoint_disc = \"/content/drive/MyDrive/FINALDATA/disc.pth.tar\"\n",
        "checkpoint_gen = \"/content/drive/MyDrive/FINALDATA/gen.pth.tar\"\n",
        "\n",
        "both_transform = A.Compose([\n",
        "    A.Resize(width=256, height=256),\n",
        "    A.HorizontalFlip(p=0.5)\n",
        "], additional_targets={\"image0\":\"image\"})\n",
        "\n",
        "transform_only_input = A.Compose(\n",
        "    [\n",
        "     A.ColorJitter(p=0.2),\n",
        "     A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
        "     A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n",
        "     ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_only_mask = A.Compose(\n",
        "    [\n",
        "     A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
        "     A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n",
        "     ToTensorV2()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "cz98gmIoM09l",
        "outputId": "a3d3ddf1-2dc4-4d7e-b591-3430cd27d61a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2d04629b37a0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Show some images from the train dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtarget_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e74b542319ba>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimg_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimg_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_GRAY2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
          ]
        }
      ],
      "source": [
        "train_dataset = ImageDataset_color(root = '/content/framesito', transforms_ = transform)\n",
        "val_dataset = ImageDataset_color(root = '/content/higher', transforms_ = transform)\n",
        "\n",
        "# Show some images from the train dataset\n",
        "for i in range(3):\n",
        "    input_image, target_image = train_dataset[i]\n",
        "    input_image = np.transpose(input_image, (1, 2, 0))\n",
        "    target_image = np.transpose(target_image, (1, 2, 0))\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(input_image)\n",
        "    axes[0].set_title('Input Image')\n",
        "    axes[1].imshow(target_image)\n",
        "    axes[1].set_title('Target Image')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7tINP5X4ED0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e5662ba-68f2-4dd7-cfba-0010c315f9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint\n",
            "=> Loading checkpoint\n"
          ]
        }
      ],
      "source": [
        "# Training:\n",
        "\n",
        "disc = Discriminator(input_channels = 3).to(device)\n",
        "gen = Generator(num_input_channels = 3, num_output_channels = 3).to(device)\n",
        "opt_disc = optim.Adam(disc.parameters(), lr = learning_rate, betas = (0.5, 0.999))\n",
        "opt_gen = optim.Adam(gen.parameters(), lr = learning_rate, betas = (0.5, 0.999))\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "\n",
        "if load_model == True:\n",
        "  load_checkpoint(checkpoint_gen, gen, opt_gen, learning_rate)\n",
        "  load_checkpoint(checkpoint_disc, disc, opt_disc, learning_rate)\n",
        "\n",
        "\n",
        "#train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = num_workers)\n",
        "g_scaler = torch.cuda.amp.GradScaler()\n",
        "d_scaler = torch.cuda.amp.GradScaler()\n",
        "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SILpy3OG71SH"
      },
      "outputs": [],
      "source": [
        "def train_fn(disc, gen, loader, opt_disc, opt_gen, l1, bce_loss, g_scaler, d_scaler):\n",
        "  loop = tqdm(loader, leave = True)\n",
        "\n",
        "  for index, (x, y) in enumerate(loop):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Train discriminator:\n",
        "    with torch.cuda.amp.autocast(): # Float 16.\n",
        "      y_fake = gen(x)\n",
        "      d_real = disc(x, y)\n",
        "      d_fake = disc(x, y_fake) # \n",
        "      d_real_loss = bce_loss(d_real, torch.ones_like(d_real))\n",
        "      d_fake_loss = bce_loss(d_fake, torch.zeros_like(d_fake))\n",
        "      d_loss = (d_real_loss + d_fake_loss) / 2 # Divide by two to make the discriminator train 'slower' than the generator. \n",
        "    \n",
        "    disc.zero_grad()\n",
        "    d_scaler.scale(d_loss).backward(retain_graph = True)\n",
        "    d_scaler.step(opt_disc)\n",
        "    d_scaler.update()\n",
        "\n",
        "    # Train generator:\n",
        "    with torch.cuda.amp.autocast():\n",
        "      d_fake = disc(x, y_fake)\n",
        "      g_fake_loss = bce_loss(d_fake, torch.ones_like(d_fake)) # Trick the discriminator into thinking that these are real images. \n",
        "      l1 = l1_loss(y_fake, y) * l1_lambda\n",
        "      g_loss = g_fake_loss + l1\n",
        "    \n",
        "    opt_gen.zero_grad() # Clear weights.\n",
        "    g_scaler.scale(g_loss).backward()\n",
        "    g_scaler.step(opt_gen)\n",
        "    g_scaler.update()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "qQwBwWZj4E0W",
        "outputId": "979c3b59-4483-49d1-9a23-e1a959f9c6f9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 33841/33841 [3:10:52<00:00,  2.95it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33841/33841 [3:05:15<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 501/33841 [02:55<3:14:26,  2.86it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-7ade53645012>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msave_model\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-2c3c19001bb7>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(disc, gen, loader, opt_disc, opt_gen, l1, bce_loss, g_scaler, d_scaler)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "  train_fn(disc, gen, train_loader, opt_disc, opt_gen, l1_loss, bce_loss, g_scaler, d_scaler)\n",
        "\n",
        "  if save_model and epoch % 1 == 0:\n",
        "    save_checkpoint(gen, opt_gen, filename = checkpoint_gen)\n",
        "    save_checkpoint(disc, opt_disc, filename = checkpoint_disc)\n",
        "  \n",
        "  save_some_examples(gen, val_loader, epoch, folder = \"./evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LubuEnJfJqGD"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def test_image(gen, img_dir, folder):\n",
        "  \"\"\"\n",
        "  Allows for testing of a single image by passing it in directly via its directory\n",
        "  and converting it to a PIL-type image. \n",
        "  The image is transformed and resized to the appropriate size.\n",
        "  Then, the input tensor is passed into the generator, which generates the 'fake'\n",
        "  colorized image and saves it in the testing folder. \n",
        "  \"\"\"\n",
        "  input_image = Image.open(img_dir).convert('RGB')\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize((1024, 1024)),\n",
        "      transforms.ToTensor(), \n",
        "      transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
        "  ])\n",
        "  input_tensor = transform(input_image).unsqueeze(0).to(device) # unsqueeze(0) \n",
        "  # adds a new dimension to the tensor at position 0. \n",
        "  # Since the transform function returns a tensor of shape (channels, height, width), we\n",
        "  # want the resulting shape to be (1, height, width) so that only one image is fed (as a batch size of 1).\n",
        "  gen.eval()\n",
        "  with torch.no_grad():\n",
        "    y_fake = gen(input_tensor)\n",
        "    y_fake = y_fake * 0.5 + 0.5 # Remove normalization.\n",
        "    save_image(y_fake, folder + f\"/colorized_image.png\")\n",
        "    save_image(input_tensor.squeeze() * 0.5 + 0.5, folder + f\"/original_image.png\")\n",
        "  gen.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTPFejVsMIsz"
      },
      "outputs": [],
      "source": [
        "test_image(gen, img_dir = \"/content/5d4d43d585600a0462410713.jpg\", folder = \"testing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dlu7MxOJTWzv"
      },
      "outputs": [],
      "source": [
        "save_some_examples(gen, val_loader, 1, \"./evaluation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR_zxMufZRYr"
      },
      "outputs": [],
      "source": [
        "load_checkpoint()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}